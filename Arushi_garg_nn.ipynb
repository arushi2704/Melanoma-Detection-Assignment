{"cells":[{"cell_type":"markdown","metadata":{"id":"yDriIbfa5lwD"},"source":["**Melanoma** **Detection** **Assignment**"]},{"cell_type":"markdown","source":["Problem statement: To build a CNN based model which can accurately detect melanoma. Melanoma is a type of cancer that can be deadly if not detected early. It accounts for 75% of skin cancer deaths. A solution which can evaluate images and alert the dermatologists about the presence of melanoma has the potential to reduce a lot of manual effort needed in diagnosis."],"metadata":{"id":"5UWi2h4ufbGi"}},{"cell_type":"markdown","metadata":{"id":"lvR7ppk77v31"},"source":["### Importing Skin Cancer Data"]},{"cell_type":"markdown","metadata":{"id":"JfcpIXQZN2Rh"},"source":["### Importing all the important libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WC8xCQuELWms"},"outputs":[],"source":["import pathlib\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import os\n","import PIL\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Sequential"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TYpVPmT5z7AP"},"outputs":[],"source":["## Mount the data to google drive by using following code :\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"My71cdCT5x9l"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"RpUsRQwOOL72"},"source":["This assignment uses a dataset of about 2357 images of skin cancer types. The dataset contains 9 sub-directories in each train and test subdirectories. The 9 sub-directories contains the images of 9 skin cancer types respectively."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D57L-ovIKtI4"},"outputs":[],"source":["# Defining the path for train and test images\n","## Todo: Update the paths of the train and test dataset\n","data_dir_train = pathlib.Path(\"/content/drive/MyDrive/Colab Notebooks/Train\")\n","data_dir_test = pathlib.Path('/content/drive/MyDrive/Colab Notebooks/Test')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DqksN1w5Fu-N"},"outputs":[],"source":["#Count of the images in Train and Test sets\n","image_count_train = len(list(data_dir_train.glob('*/*.jpg')))\n","print(image_count_train)\n","image_count_test = len(list(data_dir_test.glob('*/*.jpg')))\n","print(image_count_test)"]},{"cell_type":"markdown","metadata":{"id":"O8HkfW3jPJun"},"source":["### Load images from dataset using keras.preprocessing\n","\n","Let's load these images off disk using the helpful image_dataset_from_directory utility."]},{"cell_type":"markdown","metadata":{"id":"cDBKZG3jPcMc"},"source":["### Create a dataset\n","\n","Define some parameters for the loader:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VLfcXcZ9LjGv"},"outputs":[],"source":["#Given below is the batch size and image specification to resize images\n","batch_size = 32\n","img_height = 180\n","img_width = 180"]},{"cell_type":"markdown","metadata":{"id":"Y5f5y43GPog1"},"source":["Use 80% of the images for training, and 20% for validation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G1BWmDzr7w-5"},"outputs":[],"source":["## Write your train dataset here\n","## Note: use seed=123 while creating your dataset using tf.keras.preprocessing.image_dataset_from_directory\n","## Note, make sure your resize your images to the size img_height*img_width, while writting the dataset\n","train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","    data_dir_train,\n","    seed=123,\n","    validation_split= 0.2,\n","    subset= 'training',\n","    image_size=(img_height,img_width),\n","    batch_size = batch_size\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LYch6-SR-i2g"},"outputs":[],"source":["## Write your validation dataset here\n","## Note use seed=123 while creating your dataset using tf.keras.preprocessing.image_dataset_from_directory\n","## Note, make sure your resize your images to the size img_height*img_width, while writting the dataset\n","val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","    data_dir_train,\n","    seed=123,\n","    validation_split= 0.2,\n","    subset= 'validation',\n","    image_size=(img_height,img_width),\n","    batch_size = batch_size\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bk0RV7G7-nad"},"outputs":[],"source":["# Names of the classes of skin cancer in a list. \n","# You can find the class names in the class_names attribute on these datasets. These correspond to the directory names in alphabetical order.\n","class_names = train_ds.class_names\n","print(class_names)"]},{"cell_type":"markdown","metadata":{"id":"jbsm5oYiQH_b"},"source":["### Visualize the data\n","#### Create a code to visualize one instance of all the nine classes present in the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tKILZ48I-q1k"},"outputs":[],"source":["# You can use training or validation data to visualize\n","import matplotlib.pyplot as plt\n","plt.figure(figsize=(10,10))\n","for i in range(9): \n","  plt.subplot(3, 3, i + 1)\n","  image = plt.imread(str(list(data_dir_train.glob(class_names[i]+'/*.jpg'))[1]))\n","  plt.title(class_names[i])\n","  plt.imshow(image)"]},{"cell_type":"markdown","metadata":{"id":"8cAZPYaeQjQy"},"source":["The `image_batch` is a tensor of the shape `(32, 180, 180, 3)`. This is a batch of 32 images of shape `180x180x3` (the last dimension refers to color channels RGB). The `label_batch` is a tensor of the shape `(32,)`, these are corresponding labels to the 32 images."]},{"cell_type":"markdown","metadata":{"id":"jzVXBHiyQ7_I"},"source":["`Dataset.cache()`: It keeps the images in memory after they're loaded off disk during the first epoch.\n","\n","`Dataset.prefetch()`: It overlaps data preprocessing and model execution while training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7wZlKRBEGNtU"},"outputs":[],"source":["AUTOTUNE = tf.data.experimental.AUTOTUNE\n","train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n","val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"]},{"cell_type":"markdown","metadata":{"id":"1JEAF6-sRyz8"},"source":["### Create Model:\n","#### Create a CNN model, which can accurately detect 9 classes present in the dataset. Use ```layers.experimental.preprocessing.Rescaling``` to normalize pixel values between (0,1). The RGB channel values are in the `[0, 255]` range. This is not ideal for a neural network. Here, it is good to standardize values to be in the `[0, 1]`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ync9xoW7GZgn"},"outputs":[],"source":["from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n","num_classes = 9\n","model = Sequential([\n","                    layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width,3))\n","])\n","model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n","                 activation ='relu', input_shape = (180, 180, 32)))\n","model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n","                 activation ='relu'))\n","model.add(MaxPool2D(pool_size=(2,2)))\n","model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n","                 activation ='relu'))\n","model.add(MaxPool2D(pool_size=(2,2)))\n","model.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same', \n","                 activation ='relu'))\n","model.add(MaxPool2D(pool_size=(2,2)))\n","model.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same', \n","                 activation ='relu'))\n","model.add(MaxPool2D(pool_size=(2,2)))\n","model.add(Dropout(0.25))\n","\n","model.add(Flatten())\n","model.add(Dense(num_classes, activation = \"softmax\"))"]},{"cell_type":"markdown","metadata":{"id":"SDKzJmHwSCtt"},"source":["### Compile the model\n","Choose an appropirate optimiser and loss function for model training "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XB8wKtiPGe1j"},"outputs":[],"source":["### Choose an appropirate optimiser and loss function\n","model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ZGWN4MZGhtJ"},"outputs":[],"source":["# View the summary of all layers\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"ljD_83rwSl5O"},"source":["### Train the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kkfw2rJXGlYC"},"outputs":[],"source":["epochs = 20\n","history = model.fit(\n","  train_ds,\n","  validation_data=val_ds,\n","  epochs=epochs\n",")"]},{"cell_type":"markdown","metadata":{"id":"w3679V8OShSE"},"source":["### Visualizing training results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R1xkgk5nGubz"},"outputs":[],"source":["acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs_range = range(epochs)\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs_range, acc, label='Training Accuracy')\n","plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs_range, loss, label='Training Loss')\n","plt.plot(epochs_range, val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.title('Training and Validation Loss')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"JvPphJYuSZhK"},"source":["#### Write your findings after the model fit, see if there is an evidence of model overfit or underfit\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bNTMTRbb-GJr"},"source":["**Findings on the first base model**\n","\n","- The model is overfitting because we can also see difference in loss functions in training & test around the 10-11th epoch\n","\n","- The accuracy is around 75-80% because there are enough features to remember the pattern.\n","\n","- But again, it's too early to comment on the overfitting & underfitting debate\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"22hljAl6GykA"},"outputs":[],"source":["# Todo, after you have analysed the model fit history for presence of underfit or overfit, choose an appropriate data augumentation strategy. \n","data_augument = keras.Sequential([\n","                             layers.experimental.preprocessing.RandomFlip(mode=\"horizontal_and_vertical\",input_shape=(img_height,img_width,3)),\n","                             layers.experimental.preprocessing.RandomRotation(0.2, fill_mode='reflect'),\n","                             layers.experimental.preprocessing.RandomZoom(height_factor=(0.2, 0.3), width_factor=(0.2, 0.3), fill_mode='reflect')\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XEjPWh8GG0C7"},"outputs":[],"source":["# Todo, visualize how your augmentation strategy works for one instance of training image.\n","\n","plt.figure(figsize=(12, 12))\n","for images, labels in train_ds.take(1):\n","    for i in range(9):\n","        ax = plt.subplot(3, 3, i + 1)\n","        plt.imshow(data_augument(images)[i].numpy().astype(\"uint8\"))\n","        plt.title(class_names[labels[i]])\n","        plt.axis(\"off\")"]},{"cell_type":"markdown","metadata":{"id":"XhKDHlUdTuSX"},"source":["### Create the model, compile and train the model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W3V4l-O9G3dM"},"outputs":[],"source":["## You can use Dropout layer if there is an evidence of overfitting in your findings\n","from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n","num_classes = 9\n","model = Sequential([ data_augument,\n","                    layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width,3))\n","      \n","])\n","model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n","                 activation ='relu', input_shape = (180, 180, 32)))\n","model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n","                 activation ='relu'))\n","model.add(MaxPool2D(pool_size=(2,2)))\n","model.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same', \n","                 activation ='relu'))\n","model.add(MaxPool2D(pool_size=(2,2)))\n","model.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same', \n","                 activation ='relu'))\n","model.add(MaxPool2D(pool_size=(2,2)))\n","model.add(Dropout(0.25))\n","\n","model.add(Flatten())\n","model.add(Dense(num_classes, activation = \"softmax\"))"]},{"cell_type":"markdown","metadata":{"id":"FfUWFp96UIAN"},"source":["### Compiling the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_-7yTm8IG8zR"},"outputs":[],"source":["model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n","              metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"kC-D_RWOURp6"},"source":["### Training the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UcPfkUASHBf9"},"outputs":[],"source":["epochs=20\n","history = model.fit(\n","  train_ds,\n","  validation_data=val_ds,\n","  epochs=epochs\n",")"]},{"cell_type":"markdown","metadata":{"id":"IhNOKtSyUYzC"},"source":["### Visualizing the results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vjN_F4QxHIsh"},"outputs":[],"source":["acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs_range = range(epochs)\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs_range, acc, label='Training Accuracy')\n","plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs_range, loss, label='Training Loss')\n","plt.plot(epochs_range, val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.title('Training and Validation Loss')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"DzMltwU3B_2W"},"source":["**Finding from Second Model**\n","\n","- There is no improvement in accuracy but we can definitely see the overfitting problem has solved due to data augmentation \n","\n","- We can increase the epochs to increase the accuracy so it's too early for judgement "]},{"cell_type":"markdown","metadata":{"id":"7TdDi4u-VTkW"},"source":["#### **Todo:** Find the distribution of classes in the training dataset.\n","#### **Context:** Many times real life datasets can have class imbalance, one class can have proportionately higher number of samples compared to the others. Class imbalance can have a detrimental effect on the final model quality. Hence as a sanity check it becomes important to check what is the distribution of classes in the data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HAhwYgtTQRzq"},"outputs":[],"source":["path_list=[]\n","lesion_list=[]\n","for i in class_names:\n","      \n","    for j in data_dir_train.glob(i+'/*.jpg'):\n","        path_list.append(str(j))\n","        lesion_list.append(i)\n","dataframe_dict_original = dict(zip(path_list, lesion_list))\n","original_df = pd.DataFrame(list(dataframe_dict_original.items()),columns = ['Path','Label'])\n","original_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L0YNWshIDWpr"},"outputs":[],"source":["dataframe_dict_original = dict(zip(path_list, lesion_list))\n","original_df = pd.DataFrame(list(dataframe_dict_original.items()),columns = ['Path','Label'])\n","original_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bpmnvZK9DfHx"},"outputs":[],"source":["count=[]\n","for i in class_names:\n","    count.append(len(list(data_dir_train.glob(i+'/*.jpg'))))\n","plt.figure(figsize=(25,10))\n","plt.bar(class_names,count)"]},{"cell_type":"markdown","metadata":{"id":"4csQL1dvO0b2"},"source":["#### **Todo:** Write your findings here: \n","#### - Which class has the least number of samples?\n","#### ***Answer-***1 :- squamous cell carcinoma has least number of samples\n","\n","#### - Which classes dominate the data in terms proportionate number of samples?\n","#### ***Answer-2***:- actinic keratosis and dermatofibroma have proportionate number of classes. melanoma and pigmented benign keratosis have proprtionate number of classes\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NFArY-bvERTM"},"outputs":[],"source":["class_names"]},{"cell_type":"markdown","metadata":{"id":"Hb-stKyHPf8v"},"source":["#### **Todo:** Rectify the class imbalance\n","#### **Context:** You can use a python package known as `Augmentor` (https://augmentor.readthedocs.io/en/master/) to add more samples across all classes so that none of the classes have very few samples."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ItAg4rU-SzJh"},"outputs":[],"source":["!pip install Augmentor"]},{"cell_type":"markdown","metadata":{"id":"BZKzTe3zWL4O"},"source":["To use `Augmentor`, the following general procedure is followed:\n","\n","1. Instantiate a `Pipeline` object pointing to a directory containing your initial image data set.<br>\n","2. Define a number of operations to perform on this data set using your `Pipeline` object.<br>\n","3. Execute these operations by calling the `Pipeline’s` `sample()` method.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Egt9EHjR-Dd"},"outputs":[],"source":["import Augmentor\n","for i in class_names:\n","    p = Augmentor.Pipeline(\"/content/drive/MyDrive/Colab Notebooks/Train\",save_format='jpg')\n","    p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)\n","    p.sample(500) ## We are adding 500 samples per class to make sure that none of the classes are sparse."]},{"cell_type":"markdown","metadata":{"id":"CcBIFZGbWuFa"},"source":["Augmentor has stored the augmented images in the output sub-directory of each of the sub-directories of skin cancer types.. Lets take a look at total count of augmented images."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jxWcMqZhdRWz"},"outputs":[],"source":["data_dir_train1 = pathlib.Path(\"/content/drive/MyDrive/Colab Notebooks/Train/output\")\n","image_count_train1 = len(list(data_dir_train1.glob('*/*.jpg')))\n","print(image_count_train1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tR5IJVyvXq6Y"},"outputs":[],"source":["for i in class_names:   \n","    for j in data_dir_train1.glob(i+'/*.jpg'):\n","        path_list.append(str(j))\n","        lesion_list.append(i)\n","dataframe_dict_original = dict(zip(path_list, lesion_list))\n","new_df = pd.DataFrame(list(dataframe_dict_original.items()),columns = ['Path','Label'])\n","new_df"]},{"cell_type":"markdown","metadata":{"id":"IJ5KarKq4kWJ"},"source":["### Lets see the distribution of augmented data after adding new images to the original training data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5j45rmxd2nxK"},"outputs":[],"source":["new_df['Label'].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"9NirFBvGPmgI"},"source":["So, now we have added 500 images to all the classes to maintain some class balance. We can add more images as we want to improve training process."]},{"cell_type":"markdown","metadata":{"id":"9EnspeMbRWNs"},"source":["#### Train the model on the data created using Augmentor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hFcj1XgndRWz"},"outputs":[],"source":["batch_size = 32\n","img_height = 180\n","img_width = 180"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lXUiLrfua3e2"},"outputs":[],"source":["import pathlib\n","data_dir_train1=pathlib.Path(\"/content/drive/MyDrive/Colab Notebooks/Train\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EFwOXwakbKK7"},"outputs":[],"source":["data_dir_train1"]},{"cell_type":"markdown","metadata":{"id":"0haOU11Ey8ey"},"source":["#### **Todo:** Create a training dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"peKJYjWSnJHp"},"outputs":[],"source":["image_count_train1 = len(list(data_dir_train1.glob('*/*.jpg')))\n","print(image_count_train1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H4ZY11judRWz"},"outputs":[],"source":["import pathlib\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import os\n","import PIL\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Sequential\n","\n","data_dir_train1=pathlib.Path(\"/content/drive/MyDrive/Colab Notebooks/Train\")\n","train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","  data_dir_train1,\n","  seed=123,\n","  validation_split = 0.2,\n","  subset = \"training\",\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{"id":"mwNJVDuBP5kf"},"source":["#### **Todo:** Create a validation dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TX191d_3dRW0"},"outputs":[],"source":["val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","  data_dir_train1,\n","  seed=123,\n","  validation_split = 0.2,\n","  subset = 'validation',\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{"id":"JaoWeOEpVjqH"},"source":["#### **Todo:** Create your model (make sure to include normalization)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ch0MuKvFVr7O"},"outputs":[],"source":["## You can use Dropout layer if there is an evidence of overfitting in your findings\n","from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n","num_classes = 9\n","model = Sequential([ \n","                    layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width,3))\n","      \n","])\n","model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n","                 activation ='relu'))\n","model.add(MaxPool2D(pool_size=(2,2)))\n","model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n","                 activation ='relu'))\n","model.add(MaxPool2D(pool_size=(2,2)))\n","model.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same', \n","                 activation ='relu'))\n","model.add(MaxPool2D(pool_size=(2,2)))\n","model.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same', \n","                 activation ='relu'))\n","model.add(MaxPool2D(pool_size=(2,2)))\n","model.add(Conv2D(filters = 128, kernel_size = (5,5),padding = 'Same', \n","                 activation ='relu'))\n","model.add(MaxPool2D(pool_size=(2,2)))\n","model.add(Conv2D(filters = 128, kernel_size = (5,5),padding = 'Same', \n","                 activation ='relu'))\n","model.add(MaxPool2D(pool_size=(2,2)))\n","model.add(Dropout(0.25))\n","\n","model.add(Flatten())\n","model.add(Dense(256, activation = 'relu'))"]},{"cell_type":"markdown","metadata":{"id":"Bu5N9LxkVx1B"},"source":["#### **Todo:** Compile your model (Choose optimizer and loss function appropriately)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H47GWmLbdRW1"},"outputs":[],"source":["model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n","              metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"9gS-Y1bJV7uy"},"source":["#### **Todo:**  Train your model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fcV6OdI4dRW1"},"outputs":[],"source":["epochs =30\n","history = model.fit(\n","  train_ds,\n","  validation_data=val_ds,\n","  epochs=epochs\n",")"]},{"cell_type":"markdown","metadata":{"id":"iuvfCTsBWLMp"},"source":["#### **Todo:**  Visualize the model results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lCTXwfkTdRW1"},"outputs":[],"source":["acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs_range = range(epochs)\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs_range, acc, label='Training Accuracy')\n","plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs_range, loss, label='Training Loss')\n","plt.plot(epochs_range, val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.title('Training and Validation Loss')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Way4lakC4_p0"},"source":["#### Conclusion\n","\n"]},{"cell_type":"markdown","metadata":{"id":"b6Y5GcxkzNcJ"},"source":["- Accuracy on training data has increased by using Augmentor library\n","\n","- Model is still overfitting\n","\n","- The problem of overfitting can be solved by add more layer, neurons or adding dropout layers.\n","\n","- The Model can be further improved by tuning the hyperparameter"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Melanoma_detection_assignment.ipynb","provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}